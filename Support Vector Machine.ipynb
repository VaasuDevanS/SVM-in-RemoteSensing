{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGPzJkVi1aqW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"images/UNB.png\">\n",
    "\n",
    "# GGE 6322: IMAGE PROCESSING AND COMPUTER VISION\n",
    "## Support Vector Machine\n",
    "\n",
    "### By Vaasudevan Srinivasan presented on **March 26, 2019 09:30**\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOA6QhtI4Ilf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.) A Gentle introduction to Classification and its jargons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J01UrT5_6ZIm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Classification ?\n",
    "\n",
    "**Classification** is the problem of identifying to which of a set of categories (sub-populations) a **new observation** belongs, on the **basis of a training set of data** containing observations (or instances) whose **category membership is known.**\n",
    "\n",
    "---\n",
    "## Classifier What?\n",
    "\n",
    "An **algorithm** that implements classification, especially in a concrete implementation, is known as a **classifier**. The term \"classifier\" sometimes also refers to the **mathematical function**, implemented by a classification algorithm, that maps input data to a category. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cL8M0piDMkkO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features and Regions\n",
    "\n",
    "A crude but **functional definition** of a feature is something that can be **measured\n",
    "in an image**. A feature is therefore a number or a set of numbers derived\n",
    "from a digital image.\n",
    "\n",
    "Features are associated with **image regions**. An object within an image has\n",
    "a set of measurements (features) that can be used to characterize it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQ1t30biMvUK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training and Testing\n",
    "\"It is **standard practice** to measure and classify a set of data to establish a\n",
    "normal range for the features to be used in automatic classification. This is what\n",
    "is referred to as **training**, and it is an essential part of building a recognition.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKqBbxXV-zRH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class\n",
    "**One of a set of enumerated target values for a label**. For example, in a binary classification model that detects spam, the two classes are **spam and not spam**. In a multi-class classification model that identifies dog breeds, the classes would be **poodle, beagle, pug**, and so on.\n",
    "\n",
    "Classification = **Class** - ification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTXVHqzSGsMQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification model\n",
    "A type of machine learning model for distinguishing among two or more discrete classes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6W1MtUugGvfg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Boundary\n",
    "The **separator** between classes learned by a model in a binary class or multi-class classification problems.\n",
    "\n",
    "<img src=\"images/decision_boundary.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0p7jyv4dG2Ny",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion matrix\n",
    "An **NxN table** that summarizes how successful a classification model's predictions were..!!\n",
    "\n",
    "<center>\n",
    "<img src=\"images/ConfusionMatrix.png\" width=600 height=500>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/fp_fn.jpeg\" width=800 height=650>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Accuracy:\n",
    "The fraction of predictions that a classification model got right.\n",
    "\n",
    "$\\text{Accuracy} =\n",
    "\\frac{\\text{Correct Predictions}} {\\text{Total Number Of Examples}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Precision:\n",
    "Precision identifies the frequency with which a model was correct when predicting the positive class.\n",
    "$\\text{Precision} =\n",
    "\\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Positives}}$\n",
    "\n",
    "### Recall:\n",
    "Out of all the possible positive labels, how many did the model correctly identify?\n",
    "$\\text{Recall} =\n",
    "\\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tU7fliBES2y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.) Types of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcZOYRUBHGPq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.) Logistic Regression\n",
    "\n",
    "Logistic regression is kind of like linear regression but is used when the dependent variable is not a number, but something else (like a Yes/No response)\n",
    "\n",
    "<img src=\"images/LogisticRegression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzbIpE8ZWDZ5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.) K-Nearest Neighbours (K-NN)\n",
    "\n",
    "K-NN algorithm is one of the **simplest classification algorithm** and it is used to identify the data points that are separated into several classes to predict the classification of a new sample point. K-NN is a non-parametric, **lazy learning algorithm**. It classifies new cases based on a **similarity measure** (e.g. distance functions).\n",
    "\n",
    "Some of the distance metrics that are mentioned in the book are:\n",
    "\n",
    "* Pythagorean distance\n",
    "* Manhattan distance or city block distance\n",
    "* Mahanalobis distance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdR-MCeIWHjI",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.) Naive Bayes\n",
    "\n",
    "Naive Bayes classifier is based on Bayesâ€™ theorem with the independence assumptions between predictors.\n",
    "\n",
    "<img src=\"images/NaiveBayes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhMzqnC6WKaQ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.) Decision Tree Classification\n",
    "\n",
    "Decision tree builds **classification or regression models in the form of a tree structure**. It breaks down a dataset into **smaller and smaller subsets** while at the same time an associated decision tree is incrementally developed. The final result is a **tree with decision nodes and leaf nodes**.\n",
    "\n",
    "<img src=\"images/DecisionTrees.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFZ6r8rj4Uxv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.) Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8WqCo11ez1u",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is SVM ?\n",
    "Support Vector is used for **both regression and Classification**. It is based on the concept of decision planes that define decision boundaries. A decision plane(hyperplane) is one that separates between a set of objects having different class memberships.\n",
    "\n",
    "<img src=\"images/SupportVectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWDokiCvfd-u",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How it works ?\n",
    "It performs classification by finding the hyperplane that maximizes the margin between the two classes with the help of support vectors.\n",
    "\n",
    "<img src=\"images/Hyperplane.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xODSMxmZj4H7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernels\n",
    "\n",
    "Types of kernel function are:\n",
    "\n",
    "<img src=\"images/Kernels.gif\" width=600 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYJTNIYPwpoy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.) Paramter Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VHaVkH75J_h",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.) Code Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGN1fuY1kI9t",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the modules\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abZqy0p2kSvI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Iris Dataset\n",
    "cols = [\"SLength\", \"SWidth\", \"PLength\", \"PWidth\", \"Class\"]\n",
    "iris = pd.read_csv(\"Chap8_Datas_Code/iris-data.txt\", sep=\"\\t\", names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table> <tr>\n",
    "    <td> <h1>Setosa</h1> <img src=\"images/Iris_Setosa.jpeg\" width=400 height=400> </td>\n",
    "    <td> <h1>Veriscolor</h1> <img src=\"images/Iris_Versicolor.jpeg\" width=400 height=440> </td>\n",
    "    <td> <h1>Virginica</h1> <img src=\"images/Iris_Virginica.jpeg\" width=440 height=440> </td>\n",
    "</tr> </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.scatter((iris.SLength, iris.SWidth), (iris.PLength, iris.PWidth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scikit-learn\n",
    "from sklearn import svm, model_selection as ms\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tq0QB5kL5RMZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6.) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCAB5zpM5TOh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "*  Algorithms for Image Processing and Computer Vision Second Edition by J.R. Parker ([pdf](http://www.manalhelal.com/Books/crol/Algorithms%20for%20Image%20Processing%20and%20Computer%20Vision_2011.pdf))\n",
    "*   https://en.wikipedia.org/wiki/Statistical_classification\n",
    "*   https://developers.google.com/machine-learning/glossary/\n",
    "*   https://towardsdatascience.com/supervised-machine-learning-classification-5e685fe18a6d"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "nOA6QhtI4Ilf",
    "1tU7fliBES2y",
    "mFZ6r8rj4Uxv",
    "i8WqCo11ez1u",
    "6VHaVkH75J_h",
    "tq0QB5kL5RMZ",
    "xCAB5zpM5TOh"
   ],
   "name": "Support_Vector_Machine.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "autolaunch": true,
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
