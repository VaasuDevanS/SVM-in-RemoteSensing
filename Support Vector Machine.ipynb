{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGPzJkVi1aqW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"images/UNB.png\">\n",
    "\n",
    "# GGE 6322: IMAGE PROCESSING AND COMPUTER VISION\n",
    "## Support Vector Machine\n",
    "\n",
    "### By Vaasudevan Srinivasan presented on **March 26, 2019 09:30**\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. A Gentle Introduction to Classification and its jargons üòä\n",
    "2. Types of Classification (Supervised) ü§ê\n",
    "3. Support Vector Machines üôÇ\n",
    "4. Parameter Optimization ü§ï\n",
    "5. Code Along üòã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOA6QhtI4Ilf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Gentle introduction to Classification and its jargons üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J01UrT5_6ZIm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Classification ?\n",
    "\n",
    "**Classification** is the problem of identifying to which of a set of categories (sub-populations) a **new observation** belongs, on the **basis of a training set of data** containing observations (or instances) whose **category membership is known.**\n",
    "\n",
    "\n",
    "## Classifier What?\n",
    "\n",
    "An **algorithm** that implements classification, especially in a concrete implementation, is known as a **classifier**. The term \"classifier\" sometimes also refers to the **mathematical function**, implemented by a classification algorithm, that maps input data to a category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cL8M0piDMkkO",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Features and Regions\n",
    "\n",
    "A crude but **functional definition** of a feature is something that can be **measured in an image**. A feature is therefore a number or a set of numbers derived from a digital image.\n",
    "\n",
    "Features are associated with **image regions**. An object within an image has a set of measurements (features) that can be used to characterize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQ1t30biMvUK",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training and Testing\n",
    "\"It is **standard practice** to measure and classify a set of data to establish a normal range for the features to be used in automatic classification. This is what is referred to as **training**, and it is an essential part of building a recognition.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKqBbxXV-zRH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Class\n",
    "**One of a set of enumerated target values for a label**. For example, in a binary classification model that detects spam, the two classes are **spam and not spam**. In a multi-class classification model that identifies dog breeds, the classes would be **poodle, beagle, pug**, and so on.\n",
    "\n",
    "Classification = **Class** - ification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTXVHqzSGsMQ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification model\n",
    "A type of machine learning model for distinguishing among two or more discrete classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6W1MtUugGvfg",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Boundary\n",
    "The **separator** between classes learned by a model in a binary class or multi-class classification problems.\n",
    "\n",
    "<img src=\"images/decision_boundary.png\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0p7jyv4dG2Ny",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion matrix\n",
    "An **NxN table** that summarizes how successful a classification model's predictions were..!!\n",
    "\n",
    "<center>\n",
    "<img src=\"images/ConfusionMatrix.png\" width=600 height=500>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/fp_fn.jpeg\" width=800 height=650>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Accuracy:\n",
    "The fraction of predictions that a classification model got right.\n",
    "\n",
    "$\\text{Accuracy} =\n",
    "\\frac{\\text{Correct Predictions}} {\\text{Total Number Of Examples}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Precision:\n",
    "Precision identifies the frequency with which a model was correct when predicting the positive class.\n",
    "\n",
    "$\\text{Precision} =\n",
    "\\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Positives}}$\n",
    "\n",
    "### Recall:\n",
    "Out of all the possible positive labels, how many did the model correctly identify?\n",
    "\n",
    "$\\text{Recall} =\n",
    "\\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tU7fliBES2y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of Supervised Classification ü§ê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcZOYRUBHGPq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.) Logistic Regression\n",
    "\n",
    "Logistic regression is kind of like linear regression but is used when the dependent variable is not a number, but something else (like a Yes/No response)\n",
    "\n",
    "<img src=\"images/LogisticRegression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzbIpE8ZWDZ5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.) K-Nearest Neighbours (K-NN)\n",
    "\n",
    "K-NN algorithm is one of the **simplest classification algorithm** and it is used to identify the data points that are separated into several classes to predict the classification of a new sample point. K-NN is a non-parametric, **lazy learning algorithm**. It classifies new cases based on a **similarity measure** (e.g. distance functions).\n",
    "\n",
    "Some of the distance metrics that are mentioned in the book are:\n",
    "\n",
    "* Pythagorean distance\n",
    "* Manhattan distance or city block distance\n",
    "* Mahanalobis distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdR-MCeIWHjI",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.) Naive Bayes\n",
    "\n",
    "Naive Bayes classifier is based on Bayes‚Äô theorem with the independence assumptions between predictors.\n",
    "\n",
    "<img src=\"images/NaiveBayes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhMzqnC6WKaQ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.) Decision Tree Classification\n",
    "\n",
    "Decision tree builds **classification or regression models in the form of a tree structure**. It breaks down a dataset into **smaller and smaller subsets** while at the same time an associated decision tree is incrementally developed. The final result is a **tree with decision nodes and leaf nodes**.\n",
    "\n",
    "<img src=\"images/DecisionTrees.png\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFZ6r8rj4Uxv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines üôÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8WqCo11ez1u",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is SVM ?\n",
    "Support Vector is used for **both regression and Classification**. It is based on the concept of decision planes that define decision boundaries. A decision plane(hyperplane) is one that separates between a set of objects having different class memberships.\n",
    "<table><tr>\n",
    "    <td> <img src=\"images/SupportVectors.png\"> </td>\n",
    "    <td> <img src=\"images/Hyperplane.png\"> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Features of SVM\n",
    "* SVM attempts to optimize the **line or plane** so that it is the **best one** that can be used.\n",
    "\n",
    "* A **line** divides **two-dimensional data** into two parts; a **plane** divides **three-dimensional data** into two parts.\n",
    "\n",
    "* The **maximum margin hyperplane** is always as far from both data sets as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWDokiCvfd-u",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyper-plane ? \n",
    "It performs classification by finding the **hyperplane** that maximizes the margin between the two classes with the help of support vectors. It is a linear function that divides **N-dimensional data** into two parts.\n",
    "\n",
    "**A hyperplane in an n-dimensional Euclidean space is a flat, n-1 dimensional subset of that space that divides the space into two disconnected parts.**\n",
    "\n",
    "<img src=\"images/Hyperplane_book.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convex Hull and Support Vectors\n",
    "\n",
    "The basic idea, though, is to use feature vectors on the convex hull of the data sets as candidates to be used\n",
    "to guide the optimization. \n",
    "\n",
    "The candidates are called support vectors and are illustrated, along with the convex hulls for the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The support vectors completely define the maximal margin line, which is the line that passes as **far as possible from all three of those vectors**. There can be more than three support vectors, but not fewer.\n",
    "\n",
    "<img src=\"images/SupportVectors_book.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Linear Hyperplane ?\n",
    "\n",
    "Support vector machines can also find non-linear boundaries between classes, which is their another major advantage over other classification methods.\n",
    "\n",
    "It is accomplished by **transforming** those feature vectors so that a linear boundary can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the below example.\n",
    "\n",
    "<img src=\"images/Non_linear_ex.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transformation\n",
    "\n",
    "* Add a dimension and transform the points appropriately into a third dimension. Voila..!!\n",
    "\n",
    "<img src=\"images/Non_linear_plane.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Kernels\n",
    "\n",
    "In SVM parlance, any given transformation uses a **kernel**, which is the function that projects the data from one dimension into other dimension.\n",
    "\n",
    "<img src=\"images/Kernels.png\" width=600 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Source: http://www.youtube.com/watch?v=3liCbRZPrZA\n",
    "\n",
    "<video controls src=\"images/Kernel_Purpose.mp4\" width=700 height=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYJTNIYPwpoy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Paramter Optimisation ü§ï"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM C Parameter\n",
    "\n",
    "C Parameter controls the power between **Smooth Decision Boundary** and **Classifying points correctly**.\n",
    "\n",
    "* For large values of C, the optimization will choose a smaller-margin hyperplane.\n",
    "* A very small value of C will cause the optimizer to look for a larger-margin separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Which one is the best ?\n",
    "\n",
    "<img src=\"images/C_Parameter.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Low C is Best\n",
    "\n",
    "<img src=\"images/Best_small_C.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Wait...!!! Large C is best.\n",
    "\n",
    "<img src=\"images/Best_large_C.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hence the Trade-off but How to Find C ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The usual way to adjust the C parameter is by a **Grid search**.\n",
    "\n",
    "Typical Steps:\n",
    "* Set a range of feasible values for C, for instance C in [0,15]\n",
    "* Look for the average error using a 5 or 10 fold cross validation using the training set and keep the best value\n",
    "* Perform the same procedure but on a finer search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM Gamma Parameter\n",
    "\n",
    "It defines how far the influence of a single training example reaches. If it has a low value it means that every point has a far reach and conversely high value of gamma means that every point has close reach. \n",
    "\n",
    "Consider the below image:\n",
    "<img src=\"images/Gamma.jpeg\" width=500 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*  Imagine \"raising\" the green points, then we can separate them from the red points with a plane (hyperplane)\n",
    "*  A small gamma gives us a pointed bump in the higher dimensions, a large gamma gives us a softer, broader bump.\n",
    "\n",
    "Note: To \"raise\" the points, we must use the RBF (Radial Bias function) Kernel..!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VHaVkH75J_h",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code Along üòçüòã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGN1fuY1kI9t",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the modules\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cowsay\n",
    "\n",
    "cowsay.dragon(\"Modules are imported successfully..!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abZqy0p2kSvI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Iris Dataset\n",
    "cols = [\"SLength\", \"SWidth\", \"PLength\", \"PWidth\", \"Class\"]\n",
    "types = [\"Setosa\", \"Versicolor\", \"Virginica\"]\n",
    "iris = pd.read_csv(\"Chap8_Datas_Code/iris-data.txt\", sep=\"\\t\", names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table> \n",
    "    <tr>\n",
    "        <td style=\"text-align:center\"> <h1>Setosa</h1> </td>\n",
    "        <td style=\"text-align:center\"> <h1>Veriscolor</h1> </td>\n",
    "        <td style=\"text-align:center\"> <h1>Virginica</h1> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/Iris_Setosa.jpeg\" width=400 height=400> </td>\n",
    "        <td> <img src=\"images/Iris_Versicolor.jpeg\" width=400 height=440> </td>\n",
    "        <td> <img src=\"images/Iris_Virginica.jpeg\" width=440 height=440> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"images/Petal_Sepal.png\" width=600 height=600></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "colors = [{1:'red', 2:'blue', 3:'green'}[i] for i in iris.Class]\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Petal Length\")\n",
    "\n",
    "plt.scatter(iris.SLength, iris.PLength, c=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-learn to the rescue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Scikit-learn\n",
    "from sklearn import svm, model_selection as ms\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the Dataset into Training and Testing\n",
    "\n",
    "iris_len = pd.DataFrame([iris.SLength, iris.PLength, iris.Class]).transpose()\n",
    "\n",
    "train, test = ms.train_test_split(iris_len, test_size=0.4, random_state=100)\n",
    "cTrain, cTest = train.pop('Class'), test.pop('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Classifier\n",
    "clf = svm.SVC(gamma='auto')\n",
    "clf.fit(train, cTrain)\n",
    "\n",
    "# Predict\n",
    "predicted = clf.predict(test)\n",
    "\n",
    "print(\"Accuracy: {}%\".format(accuracy_score(cTest, predicted) *100), \"\\n\")\n",
    "print(classification_report(cTest, predicted, target_names=types))\n",
    "print(confusion_matrix(cTest, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM in Image Processing / Remote Sensing (Bonus) üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/SVM_RS.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/SVM_Applications.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCAB5zpM5TOh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "*  Algorithms for Image Processing and Computer Vision Second Edition by J.R. Parker ([pdf](http://www.manalhelal.com/Books/crol/Algorithms%20for%20Image%20Processing%20and%20Computer%20Vision_2011.pdf))\n",
    "*   https://en.wikipedia.org/wiki/Statistical_classification\n",
    "*   https://developers.google.com/machine-learning/glossary/\n",
    "*   https://towardsdatascience.com/supervised-machine-learning-classification-5e685fe18a6d\n",
    "*   https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n",
    "*   https://www.researchgate.net/post/In_support_vector_machinesSVM_how_we_adjust_the_parameter_C_why_we_use_this_parameter\n",
    "*   https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\n",
    "*   https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine\n",
    "* https://data-flair.training/blogs/applications-of-svm/\n",
    "\n",
    "\n",
    "## Python\n",
    "* <a href=\"https://scikit-learn.org/stable/\"> Scikit-learn</a> | <a href=\"https://matplotlib.org/\"> Matplotlib</a> | <a href=\"http://www.numpy.org/\"> Numpy</a> | <a href=\"https://pandas.pydata.org/\"> Pandas</a> | <a href=\"https://ipython.org/\"> Ipython</a> | <a href=\"https://github.com/VaasuDevanS/cowsay-python\"> Cowsay</a>\n",
    "\n",
    "Built with <b><a href=\"https://jupyter.org/\">Jupyter-Notebook</a></b> and hosted with <b><a href=\"https://mybinder.org/\">mybinder</a></b>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "nOA6QhtI4Ilf",
    "1tU7fliBES2y",
    "mFZ6r8rj4Uxv",
    "i8WqCo11ez1u",
    "6VHaVkH75J_h",
    "tq0QB5kL5RMZ",
    "xCAB5zpM5TOh"
   ],
   "name": "Support_Vector_Machine.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "autolaunch": true,
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
